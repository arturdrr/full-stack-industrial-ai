# Configura√ß√£o do LiteLLM Proxy para Full-Stack Industrial AI
# Documenta√ß√£o: https://litellm.vercel.app/docs/proxy/configs

# ================================
# CONFIGURA√á√ÉO DO SERVIDOR
# ================================
general_settings:
  master_key: ${LITELLM_MASTER_KEY} # Chave mestre para admin
  database_url: ${DATABASE_URL:-"sqlite:///litellm.db"} # Persist√™ncia

# Configura√ß√£o do servidor
server:
  port: 8081
  host: "0.0.0.0"
  workers: 4
  timeout: 300

# Configura√ß√µes de seguran√ßa
security:
  disable_spend_logs: false
  disable_master_key_return: true
  enforce_user_param: true

# ================================
# MODELOS E PROVEDORES
# ================================
model_list:

  # üåü ABACUS API (Principal - Cloud)
  - model_name: "gpt-4"
    litellm_params:
      model: "abacus/gpt-4"
      api_key: ${ABACUS_API_KEY}
      api_base: "https://api.abacus.ai/llm/v1"
    model_info:
      mode: "chat"
      input_cost_per_token: 0.00003
      output_cost_per_token: 0.00006
      max_tokens: 4096
      supports_function_calling: true

  - model_name: "gpt-3.5-turbo"
    litellm_params:
      model: "abacus/gpt-3.5-turbo"
      api_key: ${ABACUS_API_KEY}
      api_base: "https://api.abacus.ai/llm/v1"
    model_info:
      mode: "chat"
      input_cost_per_token: 0.000001
      output_cost_per_token: 0.000002
      max_tokens: 4096

  # üè† OLLAMA (Local - Privacidade)
  - model_name: "llama3"
    litellm_params:
      model: "ollama/llama3"
      api_base: "http://ollama.default.svc.cluster.local:11434"
    model_info:
      mode: "chat"
      max_tokens: 4096
      supports_streaming: true

  - model_name: "mistral"
    litellm_params:
      model: "ollama/mistral"
      api_base: "http://ollama.default.svc.cluster.local:11434"
    model_info:
      mode: "chat"
      max_tokens: 4096

  # ü§ó HUGGINGFACE (Open Source)
  - model_name: "codellama"
    litellm_params:
      model: "huggingface/microsoft/CodeLlama-7b-Python-hf"
      api_key: ${HUGGINGFACE_API_KEY}
    model_info:
      mode: "chat"
      max_tokens: 2048
      supports_code_completion: true

  # üîß LOCALAI (Self-hosted)
  - model_name: "localai-gpt"
    litellm_params:
      model: "localai/gpt-3.5-turbo"
      api_base: "http://localai.default.svc.cluster.local:8080/v1"
    model_info:
      mode: "chat"
      max_tokens: 4096

# ================================
# ROTAMENTO E FALLBACK
# ================================
router_settings:
  routing_strategy: "latency-based-routing" # ou "simple-shuffle"
  num_retries: 3
  timeout: 30

  # Configura√ß√£o de fallback
  fallbacks:
    - "gpt-4": ["gpt-3.5-turbo", "llama3", "mistral"]
    - "gpt-3.5-turbo": ["llama3", "mistral", "localai-gpt"]
    - "llama3": ["mistral", "localai-gpt"]

  # Health checks autom√°ticos
  health_check_interval: 30

  # Load balancing
  load_balancing:
    cooldown_time: 5
    allowed_fails: 3

# ================================
# RATE LIMITING
# ================================
litellm_settings:

  # Rate limiting por usu√°rio/chave
  max_budget: 100.0 # USD por m√™s
  budget_duration: "1mo"

  # Limites por modelo
  rpm: 600 # Requests per minute
  tpm: 60000 # Tokens per minute

  # Configura√ß√µes espec√≠ficas por provedor
  redis_host: ${REDIS_HOST:-"redis.default.svc.cluster.local"}
  redis_port: 6379
  redis_password: ${REDIS_PASSWORD}

# ================================
# CACHING
# ================================
cache:
  type: "redis" # ou "s3", "local"
  host: ${REDIS_HOST:-"redis.default.svc.cluster.local"}
  port: 6379
  password: ${REDIS_PASSWORD}

  # Configura√ß√µes de cache
  ttl: 3600 # 1 hora
  namespace: "litellm_cache"

  # Cache sem√¢ntico
  semantic_cache:
    similarity_threshold: 0.8
    cache_kwargs:
      semantic_cache: true

# ================================
# LOGGING E OBSERVABILIDADE
# ================================
logging:
  success_callback: ["prometheus", "langfuse"]
  failure_callback: ["prometheus"]

  callbacks:
    prometheus:
      service_name: "litellm_proxy"

    langfuse:
      public_key: ${LANGFUSE_PUBLIC_KEY}
      secret_key: ${LANGFUSE_SECRET_KEY}
      host: ${LANGFUSE_HOST:-"http://langfuse.default.svc.cluster.local:3000"}

  # Configura√ß√µes de log
  environment_variables:
    LITELLM_LOG: "INFO"
    LITELLM_REQUEST_TIMEOUT: 300
    LITELLM_NUM_RETRIES: 3
    LITELLM_DEBUG: false

# ================================
# SEGURAN√áA E AUTENTICA√á√ÉO
# ================================
security:
  user_api_key_auth:

    # Chaves de usu√°rio (geradas automaticamente)
    user_email_address: ${ADMIN_EMAIL:-"admin@example.com"}

  # Configura√ß√µes de autentica√ß√£o
  auth_strategy: "user_id_key_auth" # ou "api_key_auth"

  # Rate limiting por usu√°rio
  rate_limiting:
    user_max_budget: 50.0
    user_budget_duration: "1mo"

# ================================
# INTEGRA√á√ïES
# ================================
integrations:

  # Slack para alertas
  slack:
    webhook_url: ${SLACK_WEBHOOK_URL}

  # Webhook para eventos
  webhooks:
    - event: "spend_tracked"
      url: "${WEBHOOK_URL}/spend"
    - event: "key_created"
      url: "${WEBHOOK_URL}/keys"

# ================================
# CONFIGURA√á√ïES ESPEC√çFICAS
# ================================
model_specific_settings:

  # Configura√ß√µes espec√≠ficas para c√≥digo
  codellama:
    temperature: 0.1
    max_tokens: 2048
    stop: ["```", "\n\n"]

  # Configura√ß√µes para chat geral
  gpt-4:
    temperature: 0.7
    max_tokens: 4096
    presence_penalty: 0.1

  # Configura√ß√µes locais (mais liberdade)
  llama3:
    temperature: 0.8
    max_tokens: 4096
    top_p: 0.9

# ================================
# ALERTAS E MONITORAMENTO
# ================================
alerting:

  # Alertas de custo
  budget_alerts:
    - threshold: 80 # 80% do or√ßamento
      channels: ["slack", "email"]
    - threshold: 95 # 95% do or√ßamento
      channels: ["slack", "email", "webhook"]

  # Alertas de performance
  latency_alerts:
    threshold: 30 # segundos
    channels: ["slack"]

  # Alertas de falha
  error_rate_alerts:
    threshold: 5 # 5% de erro
    window: "5m"
    channels: ["slack", "webhook"]

# ================================
# DESENVOLVIMENTO/DEBUG
# ================================
development:
  enable_preview_features: true
  detailed_debug_logs: false
  mock_response_mode: false

  # M√©tricas detalhadas
  detailed_logging: true
  log_raw_request_response: false # ‚ö†Ô∏è Desabilitar em prod

# ================================
# CONFIGURA√á√ïES DE PRODU√á√ÉO
# ================================
production:
  debug_mode: false
  verbose_logging: false
  
  scaling:
    auto_scaling: true
    min_replicas: 2
    max_replicas: 10
    cpu_threshold: 70
    memory_threshold: 80
  
  backup:
    enabled: true
    interval: "0 2 * * *"  # Daily at 2 AM
    retention_days: 30